import { execSync } from 'node:child_process'
import fs from 'node:fs'
import fsPromises from 'node:fs/promises'
import path from 'node:path'
import { parseArgs } from 'node:util'
import { DeveloperDocumentId } from './migrate-id-conversion.ts'
import { unzip, zip } from './migrate-zip.ts'

main()

async function main() {
  // snapshot.zip
  const snapshotZipPath = path.resolve(import.meta.dirname, 'snapshot.zip')
  // snapshot
  const snapshotDirPath = path.resolve(import.meta.dirname, 'snapshot.temp')
  // migration.zip
  const migrationZipPath = path.resolve(import.meta.dirname, 'migration.zip')
  try {
    if (snapshotZipPath) {
      await unzip(snapshotZipPath, snapshotDirPath)
    }

    // snapshot/_components/betterAuth
    const betterAuthDirPath = path.resolve(snapshotDirPath, '_components', 'betterAuth')
    // snapshot/_components/betterAuth/_tables/documents.jsonl
    const betterAuthTablesPath = path.resolve(betterAuthDirPath, '_tables', 'documents.jsonl')
    // snapshot/_components/_tables/documents.jsonl
    const tablesFilePath = path.resolve(snapshotDirPath, '_tables', 'documents.jsonl')

    const args = parseArgs({
      args: process.argv,
      // args: process.argv.slice(2),
      allowNegative: true,
      allowPositionals: true,
      strict: false,
      options: {
        filters: {
          type: 'string',
          default: '',
        },
      },
    })

    const filters = (args.values.filters as string)
      .split(',')
      .map(f => f.trim())
      .filter(Boolean)

    const tableIDs = fs
      .readFileSync(tablesFilePath, 'utf-8')
      .split('\n')
      .filter(Boolean)
      .reduce(
        (acc, line) => {
          const table = JSON.parse(line)

          acc[table.name] = table.id as number

          return acc
        },
        {} as Record<string, number>,
      )

    const betterAuthTables = fs
      .readFileSync(betterAuthTablesPath, 'utf-8')
      .split('\n')
      .filter(Boolean)
      .map(line => {
        const table = JSON.parse(line)

        return table
      })
      .filter(table => {
        if (filters.length === 0) {
          return true
        }

        return filters.includes(table.name)
      })

    console.log('Importing BetterAuth tables:', betterAuthTables)

    const successes: string[] = []
    const errors: string[] = []

    for (const betterAuthTable of betterAuthTables) {
      const tableRowsFilePath = path.resolve(
        betterAuthDirPath,
        betterAuthTable.name,
        'documents.jsonl',
      )
      const tableRows = fs
        .readFileSync(tableRowsFilePath, 'utf-8')
        .split('\n')
        .filter(Boolean)
        .map(line => {
          return JSON.parse(line)
        })

      if (tableRows[0]) {
        const devId = DeveloperDocumentId.decode(tableRows[0]._id)
        const newDevId = new DeveloperDocumentId(tableIDs[betterAuthTable.name], devId.internalId)

        if (devId.tableNumber !== newDevId.tableNumber) {
          console.log(
            `Table ID mismatch for table ${betterAuthTable.name}: expected ${tableIDs[betterAuthTable.name]}, got ${betterAuthTable.id}`,
          )
          // errors.push(betterAuthTable.name)

          const replacements = tableRows.reduce(
            (acc, row) => {
              const devId = DeveloperDocumentId.decode(row._id)
              const newDevId = new DeveloperDocumentId(
                tableIDs[betterAuthTable.name],
                devId.internalId,
              )
              const newId = newDevId.encode()

              acc[row._id] = newId

              return acc
            },
            {} as Record<string, string>,
          )

          const destination = path.resolve(snapshotDirPath, betterAuthTable.name, 'documents.jsonl')

          fs.cpSync(tableRowsFilePath, destination)

          await replaceAllInFiles(replacements, snapshotDirPath, [betterAuthDirPath])
        }
      }
    }

    await zip(snapshotDirPath, migrationZipPath)

    const migrationCommand = `npx convex import "${migrationZipPath}" --replace -y`

    execSync(migrationCommand, {
      cwd: path.resolve(import.meta.dirname),
      stdio: 'inherit',
    })

    // for (const betterAuthTable of betterAuthTables) {
    //   const tableRowsFilePath = path.resolve(betterAuthDirPath, betterAuthTable.name, 'documents.jsonl')
    //   // const tableRowsFilePath = path.resolve(import.meta.dirname, 'deleteAll.jsonl')

    //   try {
    //     const command = `npx convex import --table ${betterAuthTable.name} "${tableRowsFilePath}" --replace -y`
    //     console.log(`Running: ${command}}`)
    //     execSync(command, {
    //       cwd: path.resolve(import.meta.dirname),
    //       stdio: 'inherit',
    //     })

    //     successes.push(betterAuthTable.name)
    //   } catch (error: any) {
    //     console.log(`Error importing table ${betterAuthTable.name}:`, error?.message)
    //     errors.push(betterAuthTable.name)
    //   }
    // }

    console.log('BetterAuth migration complete.')
    console.log('Successful imports:', successes)
    console.log('Failed imports:', errors)
  } catch (error) {
    console.error('Migration failed:', error)
  } finally {
    // fs.rmSync(snapshotDirPath, { recursive: true, force: true })
    // fs.rmSync(migrationZipPath, { force: true })
  }
}

/**
 * Recursively and concurrently replaces multiple strings in all files within a directory.
 * @param replacements - A key-value object where keys are strings to find and values are replacements.
 * @param dirPath - The path to the directory to search.
 */
async function replaceAllInFiles(
  replacements: Record<string, string>,
  dirPath: string,
  excludes?: string[],
): Promise<void> {
  try {
    // 1. Read directory entries
    const entries = await fsPromises.readdir(dirPath, { withFileTypes: true })

    // 2. Map entries to an array of Promises (Concurrent Operations)
    const operations = entries.map(async entry => {
      const fullPath = path.join(dirPath, entry.name)

      if (excludes && excludes.includes(fullPath)) {
        return
      }

      if (entry.isDirectory()) {
        // Recursive call (runs concurrently with other siblings)
        return replaceAllInFiles(replacements, fullPath, excludes)
      } else if (entry.isFile()) {
        return processFile(fullPath, replacements)
      }
    })

    // 3. Wait for all operations in this directory to complete
    await Promise.all(operations)
  } catch (error) {
    console.error(`Error processing directory ${dirPath}:`, error)
    // Depending on preference, you might want to throw here to stop execution
    // or suppress it to allow other files to finish.
    throw error
  }
}

/**
 * Helper function to process a single file.
 */
async function processFile(filePath: string, replacements: Record<string, string>): Promise<void> {
  try {
    const content = await fsPromises.readFile(filePath, 'utf-8')
    let newContent = content
    let hasChanged = false

    // Apply all replacements
    for (const [toReplace, replacement] of Object.entries(replacements)) {
      if (newContent.includes(toReplace)) {
        newContent = newContent.replaceAll(toReplace, replacement)
        hasChanged = true
      }
    }

    // Only write to disk if changes were actually made
    if (hasChanged) {
      await fsPromises.writeFile(filePath, newContent, 'utf-8')
      console.log(`Updated: ${filePath}`)
    }
  } catch (err) {
    console.error(`Failed to process file ${filePath}`, err)
  }
}
